---
title: ViT架构
tags: 神经网络
mathjax: true  
mathjax_autoNumber: true
---

论文pdf链接: [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)

# 研究思路与历史意义
这篇是比较著名的将注意力机制引入视觉领域的论文。

在当时主流研究方向是通过不断增加CNN网络的深度来提升性能（如VGG、ResNet等）。这一研究路径达到了性能瓶颈后，自然而然地就开始有研究者试图**跳脱出**CNN发展新的架构，于是从NLP领域，将注意力机制**引入**到视觉当中。

# 核心
## 图像Patch
![Patch Embed](/assets/posts/vitpatch.png){:.shadow}

为了与Transformer的token概念对齐，ViT将完整的一张图像裁剪为不重叠的若干Patch，每个Patch像素值全部输入一个线性层(或卷积层，本质都是线性映射)变换为指定长度的token向量，一个token对应一个patch。举例来讲，一张 $$3*256*256$$ 的图像，设置每个patch裁剪的长宽为 $$16$$，那么总共就可以得到 $$(256/16)*(256*16)=256$$ 个图像token。

代码如下：

```python
self.to_patch_embedding = nn.Sequential(
    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
    nn.LayerNorm(patch_dim),
    nn.Linear(patch_dim, dim),
    nn.LayerNorm(dim),
)

x = self.to_patch_embedding(img)
```
## cls token
除了从图像得到的（按上文举例的） $256$ 个token外，还会添加一个名为cls 的token（即可学习参数）,这 $257$ 个token随后输入自注意力模块中进行计算，于是每个token都能够得到与其他token的**相关程度**，如cls token会与其他所有来自图像patch的token进行计算输出，在计算时便能得到整张图像的信息，最终这个cls token单独输入给一个MLP用于输出图像的最终分类。

# 实验

# 优势

# 缺陷