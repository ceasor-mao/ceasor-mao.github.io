---
title: ViT架构
tags: 神经网络
mathjax: true  
mathjax_autoNumber: true
---

论文pdf链接: [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)

# 研究思路与历史意义
这篇是比较著名的将注意力机制引入视觉领域的论文。

在当时主流研究方向是通过不断增加CNN网络的深度来提升性能（如VGG、ResNet等）。这一研究路径达到了性能瓶颈后，自然而然地就开始有研究者试图**跳脱出**CNN发展新的架构，于是从NLP领域，将注意力机制**引入**到视觉当中。

# 核心
## 图像Patch
![Patch Embed](/assets/posts/vitpatch.png){:.shadow}

为了与Transformer的token概念对齐，ViT将完整的一张图像裁剪为不重叠的若干Patch，每个Patch像素值全部输入一个线性层(或卷积层，本质都是线性映射)变换为指定长度的token向量，一个token对应一个patch。举例来讲，一张 $$3*256*256$$ 的图像，设置每个patch裁剪的长宽为 $$16$$，那么总共就可以得到 $$(256/16)*(256*16)=256$$ 个图像token。

代码如下：

```python
self.to_patch_embedding = nn.Sequential(
    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
    nn.LayerNorm(patch_dim),
    nn.Linear(patch_dim, dim),
    nn.LayerNorm(dim),
)

x = self.to_patch_embedding(img)
```
## cls token
除了从图像得到的（按上文举例的） $$256$$ 个token外，还会添加一个名为cls 的token（即可学习参数）,这 $$257$$ 个token随后输入自注意力模块中进行计算，于是每个token都能够得到与其他token的**相关程度**，如cls token会与其他所有来自图像patch的token进行计算输出，在计算时便能得到整张图像的信息，最终这个cls token单独输入给一个MLP用于输出图像的最终分类。

# 实验
主要记录作者的实验思路，具体的数据请点击上方论文原文链接
## 实验一，比上限
比较ViT架构与同时期的SOTA 模型BiT (Big Transfer)和Noisy Student，在多个分类任务上，使用相同数据集预训练+微调的结果（注意这里的实验是在追求模型的上限，除了必要的数据集相同外，超参数、训练策略等可能都不一样），ViT效果上都要优于BiT和Noisy Student,并且ViT的训练成本（理解为算力时）也更低

## 实验二，ViT没有了CNN的归纳偏置，是否需要更多的训练数据
第一组实验，在ImageNet、ImageNet-21k、JFT-300M 三个数据集上分别预训练，微调后看 ImageNet 精度，三个数据集规模逐级增大。对于小的数据集，为了更好发挥模型性能，针对性地调整了正则化参数（weight decay, dropout, and label smoothing）。实验结果说明了在小规模数据集上ViT-Large表现不如ViT-Base，在ImageNet-21k上，两者表现相似，在最大的JFT-300M上ViT-Large更好。同时BiT在小数据集上表现也好于ViT，在大数据集上相反。通过这组实验，回答 **ViT 的"模型容量"需要足够的数据才能被激活**

第二组实验，将JFT-300M进行抽样，制作9M、30M、90M和原来的300M的数据集规模，不改变任何超参包括正则化。去掉finetune使用few-shot linear accuracy进行测试。实验结果中ViT-B/32和BiT ResNet50比较，9M样本时ViT弱，到90M时反超，后续继续扩大优势。

两组实验的区别
好比测试两种植物的耐旱性：
第一组：在沙漠、草原、雨林三种环境里分别种植，并且在沙漠里额外浇水——测的是不同环境下的综合表现
第二组：在同一块地里，严格控制浇水量为1升、3升、9升、27升，其他条件完全一样——测的是植物本身对水量的响应曲线

## 实验三，实验一的严谨版
实验一中为了追求上限，不同的模型训练策略超参经过了调参，在此实验中，每一组实验的超参都经过了固定，数据集也固定使用JFT-300M,严谨比较在不改变训练参数的情况下，模型架构和大小对效果和训练效率的影响。实验结果表明ViT的曲线优于ResNet,即能够在更小的训练成本下达到更好效果。

第二个发现是，Hybrid 在小计算量时略优于纯 ViT，大计算量时差距消失。Hybrid是指架构上，先经过卷积提取特征图，再在特征图上使用ViT。实验表明了，CNN前置提供的归纳偏执可以为ViT提供比较好的初始化，“站在巨人肩膀上”，但随着ViT参数量的增加，这一优势可以被ViT自身的架构优势抹除。

第三个发现是ViT 的性能在测试范围内没有饱和，曲线依然呈现上升趋势

## 实验四，ViT可视化
### 可视化patch embedding
实现上，每个patch的16x16x3=768维向量，经过线性层后隐射到D维，中间线性层即一个768xD的投影矩阵。对该投影矩阵进行主成分分析，提取前28个主成分，每个主成分是768维，把这28个主成分的768维重新变换为线性卷积进行可视化，观察发现这些主成分看起来像经典的图像处理基函数——类似于 Gabor 滤波器或 DCT 基，能捕捉边缘、纹理、频率等底层视觉特征，证明ViT的patch embedding与CNN的浅层的卷积层相类似，两者都自发的收敛到了同一个方向。

### 位置编码
ViT中的位置编码使用可学习参数由网络自觉学习，通过计算各 patch 位置编码之间的余弦相似度来可视化它学到的空间结构。结果是，相邻patch的位置编码相似度高，同一行同一列相似度高（学习到拓扑关系），大的参数模型位置编码能够观察到类似正弦周期模式

### 注意力
论文将注意力计算出来的权重，乘积到patch与patch之间的距离值上，以此来测量每个patch会关注距离多远的patch,来与CNN的感受野相类比。
观察发现，浅层的注意力，有些关注远距离，有些关注近距离，呈现分工规律，这也就证明了为什么ViT在较大参数时效果比Hybrid更好。在深层的注意力，则所有的注意力关注距离都趋向变大

# 优势

# 缺陷