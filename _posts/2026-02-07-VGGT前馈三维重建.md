---
title: VGGT前馈三维重建
tags: 神经网络 前馈三维重建
mathjax: true  
mathjax_autoNumber: true
---

论文pdf链接: [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/pdf/2503.11651)

# 研究思路与历史意义
CVPR2025的best paper,属于是三维重建领域的Scaling Law，通过大量的数据统一了深度、点云与位姿估计

# 核心
## 整体架构
![VGGT](/assets/posts/vggt.png){:.shadow}

VGGT的整体架构如上，主要分为两个部分，前半部分的聚合器Aggregator将图像的空间、时间信息进行计算，后半部分将Aggregator计算得到的内容分别输入给不同的Head计算位姿（相机参数）、深度、点云

代码如下：

```python
aggregated_tokens_list, patch_start_idx = self.aggregator(images)

predictions = {}

with torch.cuda.amp.autocast(enabled=False):
if self.camera_head is not None:
    pose_enc_list = self.camera_head(aggregated_tokens_list)
    predictions["pose_enc"] = pose_enc_list[-1]  # pose encoding of the last iteration
    predictions["pose_enc_list"] = pose_enc_list
    
if self.depth_head is not None:
    depth, depth_conf = self.depth_head(
        aggregated_tokens_list, images=images, patch_start_idx=patch_start_idx
    )
    predictions["depth"] = depth
    predictions["depth_conf"] = depth_conf

if self.point_head is not None:
    pts3d, pts3d_conf = self.point_head(
        aggregated_tokens_list, images=images, patch_start_idx=patch_start_idx
    )
    predictions["world_points"] = pts3d
    predictions["world_points_conf"] = pts3d_conf
```

## Alternating-Attention
Alternating-Attention(AA)是在Aggregator中用于处理时间、空间注意力的部分，分为两部分 帧内注意力与帧间注意力，简单来说就是把注意力计算在时间维度和空间维度交替进行一次。

```python
def _process_frame_attention(self, tokens, B, S, P, C, frame_idx, pos=None):
        """
        Process frame attention blocks. We keep tokens in shape (B*S, P, C).
        """
        # If needed, reshape tokens or positions:
        if tokens.shape != (B * S, P, C):
            tokens = tokens.view(B, S, P, C).view(B * S, P, C)

        if pos is not None and pos.shape != (B * S, P, 2):
            pos = pos.view(B, S, P, 2).view(B * S, P, 2)

        intermediates = []

        # by default, self.aa_block_size=1, which processes one block at a time
        for _ in range(self.aa_block_size):
            if self.training:
                tokens = checkpoint(self.frame_blocks[frame_idx], tokens, pos, use_reentrant=self.use_reentrant)
            else:
                tokens = self.frame_blocks[frame_idx](tokens, pos=pos)
            frame_idx += 1
            intermediates.append(tokens.view(B, S, P, C))

        return tokens, frame_idx, intermediates
```

帧内注意力，即在空间上计算注意力，重点关注`tokens = tokens.view(B, S, P, C).view(B * S, P, C)`这行，将B和时间维度S合并，注意力计算发生在P维度，即同一张图像上的token上。

```python
def _process_global_attention(self, tokens, B, S, P, C, global_idx, pos=None):
        """
        Process global attention blocks. We keep tokens in shape (B, S*P, C).
        """
        if tokens.shape != (B, S * P, C):
            tokens = tokens.view(B, S, P, C).view(B, S * P, C)

        if pos is not None and pos.shape != (B, S * P, 2):
            pos = pos.view(B, S, P, 2).view(B, S * P, 2)

        intermediates = []

        # by default, self.aa_block_size=1, which processes one block at a time
        for _ in range(self.aa_block_size):
            if self.training:
                tokens = checkpoint(self.global_blocks[global_idx], tokens, pos, use_reentrant=self.use_reentrant)
            else:
                tokens = self.global_blocks[global_idx](tokens, pos=pos)
            global_idx += 1
            intermediates.append(tokens.view(B, S, P, C))

        return tokens, global_idx, intermediates
```

帧间注意力，即在时间上计算注意力，重点关注`tokens = tokens.view(B, S, P, C).view(B, S * P, C)`该行，可以发现，除了该行外其他代码与帧内注意力无异。将时间维度和空间维度P合并，注意力计算发生在S*P上，因此实际上，除了时间信息，注意力计算还包含进了空间维度。

# 实验

# 优势

# 缺陷