---
title: DPT在密集任务上使用ViT
tags: 神经网络 深度估计 图像分割
mathjax: true  
mathjax_autoNumber: true
---

论文pdf链接: [Vision Transformers for Dense Prediction](https://arxiv.org/pdf/2103.13413)

# 研究思路与历史意义
当前(2021年)几乎所有密集预测架构都基于卷积网络，且普遍遵循encoder-decoder的设计模式，且学术研究焦点集中在对decoder和聚合机制上，而该文章则从backbone入手，认为骨干网络的选择对整体模型能力影响更大，因此提出了DPT，核心动作是把 ViT 作为编码器骨干。

作者对encoder下采样的观点比较有趣，认为深层特征的空间分辨率和细粒度信息不可逆地丢失，对于图像分类来说无所谓，但密集预测需要像素级的输出，分辨率损失是致命的

# 核心
## Transformer encoder
1. 注意力机制的token在整个计算过程中数量恒定，使得信息丢失的问题得到解决，如一个patch的像素数为$16*16*3=768$，而token一般可以到768/1024,意味着线性投影在理论上有足够的容量保留每个 patch 内的所有像素级信息
2. 注意力机制自带的全局感受能力，
3. 使用ViT-Hybrid架构，将线性投影替换为ResNet50的1/16分辨率的特征图，此处的观点与ViT论文有所不同，ViT认为在大参数模型下直接使用线性投影能够更好的减少约束，而此处则是推崇使用ResNet这样的约束，两者在语境下有两处不同，其一是任务不同，ViT针对分类，DPT针对密集预测，其二是数据规模不同，可能在DPT的数据集下Hybrid架构更有效，论文后续也针对此进行了消融实验

## cls token
ViT本身借鉴了BERT使用了cls token来捕获全局信息，但是该token在密集估计任务中定位比较特殊，其没有具体的空间位置，论文比对了三种处理方式，一是直接丢弃，二是直接加到其他token上，三是拼接到每个patch token上再经过MLP处理

实验观察第三种方式效果最好，第二种方式甚至不如第一种

## Convolutional decoder
论文在decoder上直接复用卷积的decoder,这就导致transformer encoder部分需要模仿先前的特征金字塔

为了实现金字塔，针对不同深度的transformer层得到的token，其实得到的都是24x24xD，应用普通卷积和转置卷积模拟下采样和上采样，对其到需要的特征图大小，得到的四个不同分辨率的特征图，用 RefineNet 风格的融合模块从小到大逐步合并。

# 实验

# 优势

# 缺陷