---
title: FastVGGT分析全局注意力
tags: 神经网络 前馈三维重建
mathjax: true  
mathjax_autoNumber: true
---

论文pdf链接: [FastVGGT: Training-Free Acceleration of Visual Geometry Transformer](https://arxiv.org/pdf/2509.02560)

# 研究思路与历史意义
FastVGGT是对VGGT的注意力图进行分析后发现存在大量的特征冗余，发生了token 坍塌，即token之间趋于相似，导致patch token之间无法被区分，局部特征退化。在此发现上通过设计token merging的方法，压缩模型token，加快模型推理，其对注意力机制的分析是非常值得学习的。

# 核心
## 注意力图可视化
在VGGT的模型结构中，token的排列顺序是`[1 * camera_token 4 * register_token, n * patch_tokens]`，`camera_token`用于估计相机位姿，`register_token`用于融合全局信息，算上时间，因此到达全局注意力的token数量为$T*(5+n)$

```python
for target_token_idx in [
    0,
    self.patch_height * self.patch_width,
    self.patch_height * self.patch_width * 10,
]:  # Iterate through each image's target_token
    pass
```

由上面分析可知，提取了第0帧的camera token和两个图像的patch token。可视化注意力图后，发现同一个block中的不同token的注意力图非常相似，每个token的关注点都是相似的。

## Token Partitioning
通过压缩token数量来1. 加速计算 2. 缓解特征退化 token坍塌 注意力相似 问题

步骤一，通过固定步长，将所有token的10%作为Salient token，固定步长相比于top-k能够在不增加过多计算量的同时保留效果。Salient token被排除在merging操作中，直接保留原样输入注意力计算
```python
if enable_protection:
    num_protected = int(N * 0.1)          # 保护全部 token 数量的 10%
    step = max(1, N // num_protected)      # 固定步长采样，均匀分布在序列中
    protected_indices = torch.arange(0, N, step, device=metric.device)[:num_protected]
```
步骤二，将第一帧所有token,其他帧的所有camera token register token标记为dst
```python
idx_buffer_seq[:tokens_per_img] = -1  # 把第一帧所有token设置为-1,即标记dst token

# 其他帧的camera_token和register_token也设置为dst token
cls_indices = (
    torch.arange(1, num_imgs, device=metric.device) * tokens_per_img
)  # 每帧起始位置（camera token 的全局索引）
cls_indices = cls_indices[:, None] + torch.arange(5, device=metric.device)
# 每帧加上偏移 0,1,2,3,4 → 得到 camera+register 共 5 个 token 的索引
idx_buffer_seq[cls_indices.flatten()] = -1  # 全部标记为 dst
```

步骤三，随机选择dst，将patch token划分区域（如2*2），每个区域抽取一个dst，继续放置到idx_buffer_seq中，其他的则作为src

步骤四，通过余弦相似度，选择每个src最相似的dst，将匹配到的token合并，需要注意的是，每个src一定对应一个dst,但是可能会有多个src对应到同一个dst,此时则将这些匹配的src和dst进行相加求平均

步骤五，将过滤压缩后的token重新计算注意力

步骤六，将计算得到的新token复制放置到所有匹配位置，还原。

# 实验

# 优势

# 缺陷